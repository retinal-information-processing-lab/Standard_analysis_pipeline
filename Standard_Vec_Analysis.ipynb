{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998db494",
   "metadata": {},
   "source": [
    "# Vec File Based Analysis\n",
    "\n",
    "This notebook allows you to perform a simple \"classic analysis of raster and psth of your recording using you vec file as reference for sequences. Your vec file last column must contain the trigger sequence key. This shold have the following shape : \"1\" + \"any number you want\" + \"Two digit repetition number\". Repetition number don't need to be sorted. They will be analysed and stacked in the raster in their order of appearance in the vec file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46407c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import params\n",
    "from utils import *\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# For clustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy as sc\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from collections import defaultdict\n",
    "\n",
    "# Automatic raw files detection\n",
    "def find_files(path):\n",
    "    return sorted([os.path.splitext(f)[0] for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f)) and os.path.splitext(f)[1] == \".mcd\")])       #If no, the path is considered as a folder and return the name of all the files in alphabetic order\n",
    "\n",
    "\n",
    "def split_spikes_between_triggers(spike_train,triggers):\n",
    "    \"\"\"\n",
    "Returns a list of spikes includes between 2 triggers in a row. Everything must be in sampling point or sec.\n",
    "    \"\"\"\n",
    "    return [spike_train[(spike_train >= triggers[i]) & (spike_train < triggers[i + 1])] for i in range(len(triggers) - 1)]\n",
    "\n",
    "def get_sequences_triggers(triggers, vec):\n",
    "    \"\"\"\n",
    "Spilt all triggers into dict of triggers from the same sequence using the key on the last colomn of the vec.\n",
    "Same key for the triggers means same sequence.\n",
    "\n",
    "Could be rewritten without the \"defaultdict\" trick\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    sequences = defaultdict(list)\n",
    "\n",
    "    keys = vec[:, -1].astype(int).astype(str)\n",
    "    for key, trigger in zip(keys, triggers):\n",
    "        sequences[key].append(trigger)\n",
    "    \n",
    "    return dict(sequences) #this dictionnary has its keys ordered as the vec. !! CAUTION !! works for python > 3.7 only\n",
    "   \n",
    "    \n",
    "def get_spikes_sequences(spike_times, trig_seq):\n",
    "    \"\"\"\n",
    "Read the first trigger of all sequence and group all spikes between each begining of sequence into a dict with\n",
    "sequence key as dict key and a list of spike times with the 0 at the begining of a sequence.\n",
    "    \"\"\"\n",
    "    trigs=[[trig_list[0],trig_list[-1]+np.mean(np.diff(np.array(trig_list)))] for trig_list in trig_seq.values()]  #make a list of all first and last trig of each seq    \n",
    "    splited_spikes = [split_spikes_between_triggers(spike_times,seq_times)[0] for seq_times in trigs]\n",
    "    return dict(zip(trig_seq.keys(), splited_spikes))\n",
    "    \n",
    "    \n",
    "def spikeseq2raster(spikesequences, trig_seq):\n",
    "    \"\"\"\n",
    "Makes a raster from a dictionnary of sequences splited with repetition. \n",
    "Looks for the key to stack repetitions (last 2 digits of the key). Repetition number is not representative of when it has been played \n",
    "    \"\"\"\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    rasters = defaultdict(list) #more compliant than dict. Allows you to either use an existing key or create it with empty list and than use it if missing.\n",
    "\n",
    "    for key in spikesequences.keys():\n",
    "        rasters[key[1:-2]].append(spikesequences[key]-trig_seq[key][0])\n",
    "    return dict(rasters)\n",
    "\n",
    "def spikeseq2psth(raster, trig_seq, n_bin=40):\n",
    "    psth={}\n",
    "    for key in raster.keys():\n",
    "        n_rep = len(raster[key])\n",
    "        if key=='':\n",
    "            seq_range  = (0, trig_seq['0'][-1]-trig_seq['0'][0] + np.mean(np.diff(trig_seq['0'])))\n",
    "        else:\n",
    "            seq_range  = (0, trig_seq['1'+key+'00'][-1]-trig_seq['1'+key+'00'][0] + np.mean(np.diff(trig_seq['1'+key+'00'])))\n",
    "        \n",
    "        if n_bin ==\"relative\":\n",
    "            all_spikes_times=[]\n",
    "            for i in range(n_rep):\n",
    "                all_spikes_times+=list(raster[key][i])\n",
    "            psth[key] = np.histogram(np.array(all_spikes_times), bins=max(1,int(np.sqrt(len(all_spikes_times)))), range=seq_range   )[0]/n_rep\n",
    "        else:\n",
    "            binned_spike_count = np.zeros((n_rep, n_bin))\n",
    "            for i in range(n_rep):\n",
    "                binned_spike_count[i,:] = np.histogram(raster[key][i], bins=n_bin, range=seq_range   )[0]\n",
    "            psth[key] = np.sum(binned_spike_count, axis=0)/n_rep\n",
    "            \n",
    "    return psth\n",
    "\n",
    "def smooth(scalars: list[float], weight: float) -> list[float]:  # Weight between 0 and 1\n",
    "    \"\"\"\n",
    "Function to smooth a 1D numpy array before plotting\n",
    "    \"\"\"\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "        \n",
    "    return smoothed\n",
    "\n",
    "def reshape_dict(original_dict):\n",
    "    \"\"\"\n",
    "This function allows you to reshape dictionnaries by reversing their keys. \n",
    "If you have {Cell1 : {key1: data, key2: data}, Cell2 : {key1: data, key2: data}}\n",
    "you will get {key1 : {Cell1: data, Cell2: data}, key2 : {Cell1: data, Cell2: data}}\n",
    "    \"\"\"\n",
    "    reshaped_dict = {}\n",
    "\n",
    "    for cell_number, seq_dict in original_dict.items():\n",
    "        for seq_number, data_dict in seq_dict.items():\n",
    "            if seq_number not in reshaped_dict:\n",
    "                reshaped_dict[seq_number] = {}\n",
    "            reshaped_dict[seq_number][cell_number] = data_dict\n",
    "\n",
    "    return reshaped_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f03ef1",
   "metadata": {},
   "source": [
    "### Load recording data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293162c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Variables\n",
    "    \n",
    "    DO NOT CHANGE VALUES HERE UNLESS DEBUG/SPECIFIC USE\n",
    "    \n",
    "    All the variables used in this part of the cell should always refere to your 'params.py' file\n",
    "    unless you want to manually change them only for this run (i.e. debugging). \n",
    "    You may have to add those variable into the function you want to adapt as only the minimal \n",
    "    amount of var are currently given to functions as inputs.\n",
    "\"\"\"\n",
    "\n",
    "recording_names = params.recording_names\n",
    "\n",
    "#Experiment name\n",
    "exp = params.exp\n",
    "\n",
    "#Analysis output directory\n",
    "output_directory=params.output_directory\n",
    "\n",
    "#Sampling rate of the mea\n",
    "fs = params.fs  \n",
    "\n",
    "#Trigger directory\n",
    "triggers_directory= params.triggers_directory\n",
    "\n",
    "#Vec files drectory\n",
    "vec_directory = os.path.join(params.root,'VEC_Files')\n",
    "\n",
    "#List all vec files in vec files directory\n",
    "available_vec = os.listdir(os.path.normpath(vec_directory))\n",
    "\n",
    "\"\"\"\n",
    "    Input\n",
    "\"\"\"\n",
    "\n",
    "cells_to_skip = []  ### LOAD HERE CELLS TO SKIP IF YOU WANT TO SELECT SPECIFICALY SOME OF THEM\n",
    "\n",
    "\n",
    "#Find rec triggers\n",
    "print(*['{} : {}'.format(i,recording_name) for i, recording_name in enumerate(recording_names)], sep=\"\\n\")\n",
    "recording_number = int(input(\"\\nSelect recording : \"))\n",
    "rec = recording_names[recording_number]\n",
    "print(f\"\\nSelected recording : {rec} \\n\")\n",
    "\n",
    "analysis_directory = os.path.normpath(os.path.join(output_directory,r'Vec_Analysis_rec_{}'.format(recording_number)))\n",
    "if not os.path.isdir(analysis_directory): os.makedirs(analysis_directory)\n",
    "\n",
    "#Find stim vec file\n",
    "print(*['{} : {}'.format(i,vec_file) for i, vec_file in enumerate(available_vec)], sep=\"\\n\")\n",
    "vec_number = int(input(\"\\nSelect stimulus file : \"))\n",
    "\n",
    "\"\"\"\n",
    "    Processing\n",
    "\"\"\"\n",
    "\n",
    "#Load vec file\n",
    "vec = np.loadtxt(os.path.join(vec_directory,available_vec[vec_number]))[1:,:]  #Remove first line as it is not a trigger\n",
    "print(f\"\\nSelected vec file : {available_vec[vec_number]}\\n\")\n",
    "print(f\"Vec file length : {vec.shape[0]}\")\n",
    "\n",
    "#load triggers\n",
    "trig_data = load_obj(os.path.normpath(os.path.join(triggers_directory,'{}_{}_triggers.pkl'.format(exp,rec))))\n",
    "trig_indices = trig_data['indices']\n",
    "stim_onsets = trig_data['indices']/fs\n",
    "print(f\"Total triggers number : {len(stim_onsets)}\")\n",
    "print(f\"Triggers type loaded : {trig_data['trigger_type']}\")\n",
    "\n",
    "spike_trains=load_obj(os.path.join(output_directory, r'{}_fullexp_neurons_data.pkl'.format(exp)))\n",
    "\n",
    "cells=list(spike_trains.keys())\n",
    "spike_times={}\n",
    "for cell in cells:\n",
    "    if cell in cells_to_skip: continue\n",
    "    spike_times[cell] = (spike_trains[cell][rec])\n",
    "    \n",
    "print('Total : {} neurons loaded \\n\\nClusters id :\\n{}\\n'.format(len(spike_trains.keys()),cells))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc6167",
   "metadata": {},
   "source": [
    "### Compute raster and psth for all sequences types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Input\n",
    "\"\"\"\n",
    "\n",
    "n_bin=40          # binning for the psth, default = 40\n",
    "n_bin ='relative' # if n_bin = \"relative\", n_bin = sqrt(nb of spikes in the raster)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Variables\n",
    "\"\"\"\n",
    "\n",
    "dict_name = f'{rec}_dict.pkl'\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Processing\n",
    "\"\"\"\n",
    "\n",
    "#Check if computing is needed first\n",
    "if not os.path.exists(os.path.join(analysis_directory, dict_name)) or (str(input('Trigers already extracted previously. Write again files files? Type Yes to do so :\\n')) in [\"Y\", \"y\", \"yes\", \"Yes\"]):\n",
    "    print(\"Splitting spikes into sequences for each cluster...\")\n",
    "    rec_dict = {}\n",
    "    sorted_data = {}\n",
    "    trig_seq  = get_sequences_triggers(stim_onsets, vec) # Create a dictionnary with sequence keys including repetion number\n",
    "                                                         # as key and a list of all the triggers in this repetition seq\n",
    "                                                         # {\"rep_seq_key\" : [float]}\n",
    "    for i in tqdm(cells):\n",
    "\n",
    "        #Process\n",
    "        rec_dict[i]={}\n",
    "        spike_seq = get_spikes_sequences(spike_times[i], trig_seq) # Create a dictionnary for each cluster with sequence keys including repetion number\n",
    "                                                                   # as key and a list of all the spikes in this repetition seq \n",
    "                                                                   # {\"rep_seq_key\" : [float]}\n",
    "                \n",
    "        raster    = spikeseq2raster(spike_seq, trig_seq)           # Create a dictionnary for each cluster with sequence types keys \n",
    "                                                                   # as key and a list of all the repetition of this sequence type (raster of the sequence type) \n",
    "                                                                   # {\"seq_type_key\" : [np.arrays]}\n",
    "\n",
    "        psth      = spikeseq2psth(raster, trig_seq, n_bin=n_bin)   # Create a dictionnary for each cluster with sequence types keys \n",
    "                                                                   # as key and a binning of the raster \n",
    "                                                                   # {\"seq_type_key\" : np.array (len(n_bin))}\n",
    "                \n",
    "                \n",
    "        #Cluster all data in a dict {Cell_id : {\"seq_type_key\": {'raster': [np.arrays]; 'psth':np.array (len(n_bin)) }  }   }\n",
    "        for key in raster.keys():\n",
    "            if key=='':\n",
    "                continue\n",
    "            rec_dict[i][key]={}\n",
    "            rec_dict[i][key]['raster']   = raster[key]\n",
    "            rec_dict[i][key]['psth']     = psth[key]\n",
    "            \n",
    "            #Add the necessary info for plotting about the seq length and triggers taking the repetition number 0 of this seqence type\n",
    "            rec_dict[i][key]['triggers'] = {\n",
    "                'start':trig_seq['1'+key+'00'][0], \n",
    "                'end':trig_seq['1'+key+'00'][-1], \n",
    "                'rng':(0, trig_seq['1'+key+'00'][-1]-trig_seq['1'+key+'00'][0] + np.mean(np.diff(trig_seq['1'+key+'00'])))\n",
    "                 }   \n",
    "            \n",
    "        sorted_data[i] = spike_seq\n",
    "\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "    Saving\n",
    "\"\"\"\n",
    "\n",
    "    print(\"Saving...\")\n",
    "    save_obj(rec_dict, os.path.join(analysis_directory, dict_name))\n",
    "    save_obj(trig_seq, os.path.join(analysis_directory, f'{rec}_Sorted_triggers.pkl'))\n",
    "    save_obj(sorted_data, os.path.join(analysis_directory, f'{rec}_Sorted_Spikes.pkl'))\n",
    "else:\n",
    "    print(f\"Dictionnary loaded from : \\n{os.path.join(analysis_directory, dict_name)}\")\n",
    "    rec_dict = load_obj(os.path.join(analysis_directory, dict_name))\n",
    "    \n",
    "del sorted_data \n",
    "print(\"----- Done -----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d3bd7",
   "metadata": {},
   "source": [
    "### Plot Raster and PSTH for all conditions (cell x sequence types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b892bf6",
   "metadata": {},
   "source": [
    "Can take a lot of time (about 10s per condition). You may want to load the rec_dict in an other notebook to do your own plotting for it to be more efficient !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da25187e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters_as_folder = True   #If true, creates a folder per cell and save there a plot of this cell for each sequence\n",
    "                            #else, creates a folder per sequence and save there a plot of this seq for each cell\n",
    "\n",
    "color =\"#B85A8F\"     #Plotting color\n",
    "\n",
    "if clusters_as_folder:\n",
    "    dict_to_plot = rec_dict\n",
    "    element = \"Cell\"\n",
    "    scd_element = \"Sequence\"\n",
    "\n",
    "else:\n",
    "    dict_to_plot = reshape_dict(rec_dict)  #Reverse dict in the case of saving per sequence\n",
    "    element = \"Sequence\"\n",
    "    scd_element = \"Cell\"\n",
    "\n",
    "for elt in tqdm(dict_to_plot.keys()):\n",
    "    item_directory = os.path.normpath(os.path.join(analysis_directory,f'{element}_{elt}')) #Saving folder for this cell or sequence\n",
    "    if not os.path.isdir(item_directory): os.makedirs(item_directory)\n",
    "    \n",
    "    for scd_elt in dict_to_plot[elt].keys():\n",
    "        if os.path.isfile(os.path.join(item_directory,f'{scd_element}_{scd_elt}.png')): continue  #Check if need to plot or not\n",
    "        \n",
    "        # New figure\n",
    "        fig, axs = plt.subplots(nrows = 2,ncols = 1, sharex=True, gridspec_kw={'height_ratios': [3, 1]}, figsize=(10,10))\n",
    "                \n",
    "        #Plot the rasters\n",
    "        ax_rast = axs[0]\n",
    "        ax_rast.eventplot(dict_to_plot[elt][scd_elt][\"raster\"], color=color)\n",
    "        ax_rast.set(title = \"Raster plot\", ylabel='N Repetitions')\n",
    "            \n",
    "        #Plot the psth\n",
    "        ax_psth = axs[1]\n",
    "        y    = dict_to_plot[elt][scd_elt][\"psth\"]\n",
    "        rng  = dict_to_plot[elt][scd_elt]['triggers']['rng']\n",
    "        y    = y*(len(y)/(rng[1]-rng[0]))   #Turning number of spikes into firing rate \n",
    "        ysmooth = smooth(y,.4)\n",
    "        \n",
    "        x=np.linspace(rng[0],rng[1],len(y))\n",
    "        ax_psth.fill_between(x, ysmooth,0,alpha=1, color=color)\n",
    "        ax_psth.set(xlabel='Time in sec', ylabel='Firing rate (spikes/s)')\n",
    "        ax_psth.set_ylim(bottom=-0.1, top=max(1, max(ysmooth)))\n",
    "\n",
    "        #Finish the plot and save\n",
    "        plt.suptitle(f'{scd_element}_{scd_elt}')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.savefig(os.path.join(item_directory,f'{scd_element}_{scd_elt}.png'))\n",
    "        plt.close(fig)\n",
    "        del fig\n",
    "        gc.collect() #Just in case..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
