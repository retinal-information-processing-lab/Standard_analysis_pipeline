{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7ed1d6",
   "metadata": {},
   "source": [
    "# Vec File Based Analysis\n",
    "\n",
    "This notebook allows you to perform a simple \"classic analysis of raster and psth of your recording using you vec file as reference for sequences. Your vec file last column must contain the trigger sequence key. This shold have the following shape : \"1\" + \"any number you want\" + \"Two digit repetition number\". Repetition number don't need to be sorted. They will be analysed and stacked in the raster in their order of appearance in the vec file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00750495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "# matplotlib.use('qt5agg')\n",
    "import params\n",
    "from utils import *\n",
    "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
    "\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# For clustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy as sc\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from collections import defaultdict\n",
    "\n",
    "# Automatic raw files detection\n",
    "def find_files(path):\n",
    "    return sorted([os.path.splitext(f)[0] for f in os.listdir(path) if (os.path.isfile(os.path.join(path, f)) and os.path.splitext(f)[1] == \".mcd\")])       #If no, the path is considered as a folder and return the name of all the files in alphabetic order\n",
    "\n",
    "\n",
    "def split_spikes_between_triggers(spike_train,triggers):\n",
    "    \"\"\"\n",
    "Returns a list of spikes includes between 2 triggers in a row. Everything must be in sampling point or sec.\n",
    "    \"\"\"\n",
    "    return [spike_train[(spike_train >= triggers[i]) & (spike_train < triggers[i + 1])] for i in range(len(triggers) - 1)]\n",
    "\n",
    "def get_sequences_triggers(triggers, vec):\n",
    "    \"\"\"\n",
    "Spilt all triggers into dict of triggers from the same sequence using the key on the last colomn of the vec.\n",
    "Same key for the triggers means same sequence.\n",
    "\n",
    "Could be rewritten without the \"defaultdict\" trick\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    sequences = defaultdict(list)\n",
    "\n",
    "    keys = vec[:, -1].astype(int).astype(str)\n",
    "    for key, trigger in zip(keys, triggers):\n",
    "        sequences[key].append(trigger)\n",
    "    \n",
    "    return dict(sequences) #this dictionnary has its keys ordered as the vec. !! CAUTION !! works for python > 3.7 only\n",
    "   \n",
    "    \n",
    "def get_spikes_sequences(spike_times, trig_seq):\n",
    "    \"\"\"\n",
    "Read the first trigger of all sequence and group all spikes between each begining of sequence into a dict with\n",
    "sequence key as dict key and a list of spike times with the 0 at the begining of a sequence.\n",
    "    \"\"\"\n",
    "    trigs=[[trig_list[0],trig_list[-1]+np.mean(np.diff(np.array(trig_list)))] for trig_list in trig_seq.values()]  #make a list of all first and last trig of each seq    \n",
    "    splited_spikes = [split_spikes_between_triggers(spike_times,seq_times)[0] for seq_times in trigs]\n",
    "    return dict(zip(trig_seq.keys(), splited_spikes))\n",
    "    \n",
    "    \n",
    "def spikeseq2raster(spikesequences, trig_seq):\n",
    "    \"\"\"\n",
    "Makes a raster from a dictionnary of sequences splited with repetition. \n",
    "Looks for the key to stack repetitions (last 2 digits of the key). Repetition number is not representative of when it has been played \n",
    "    \"\"\"\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    rasters = defaultdict(list) #more compliant than dict. Allows you to either use an existing key or create it with empty list and than use it if missing.\n",
    "\n",
    "    for key in spikesequences.keys():\n",
    "        rasters[key[1:-2]].append(spikesequences[key]-trig_seq[key][0])\n",
    "    return dict(rasters)\n",
    "\n",
    "def spikeseq2psth(raster, trig_seq, n_bin=40):\n",
    "    psth={}\n",
    "    for key in raster.keys():\n",
    "        n_rep = len(raster[key])\n",
    "        if key=='':\n",
    "            seq_range  = (0, trig_seq['0'][-1]-trig_seq['0'][0] + np.mean(np.diff(trig_seq['0'])))\n",
    "        else:\n",
    "            seq_range  = (0, trig_seq['1'+key+'00'][-1]-trig_seq['1'+key+'00'][0] + np.mean(np.diff(trig_seq['1'+key+'00'])))\n",
    "        \n",
    "        if n_bin ==\"relative\":\n",
    "            all_spikes_times=[]\n",
    "            for i in range(n_rep):\n",
    "                all_spikes_times+=list(raster[key][i])\n",
    "            psth[key] = np.histogram(np.array(all_spikes_times), bins=max(1,int(np.sqrt(len(all_spikes_times)))), range=seq_range   )[0]/n_rep\n",
    "        else:\n",
    "            binned_spike_count = np.zeros((n_rep, n_bin))\n",
    "            for i in range(n_rep):\n",
    "                binned_spike_count[i,:] = np.histogram(raster[key][i], bins=n_bin, range=seq_range   )[0]\n",
    "            psth[key] = np.sum(binned_spike_count, axis=0)/n_rep\n",
    "            \n",
    "    return psth\n",
    "\n",
    "def smooth(scalars: list[float], weight: float) -> list[float]:  # Weight between 0 and 1\n",
    "    \"\"\"\n",
    "Function to smooth a 1D numpy array before plotting\n",
    "    \"\"\"\n",
    "    last = scalars[0]  # First value in the plot (first timestep)\n",
    "    smoothed = list()\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # Calculate smoothed value\n",
    "        smoothed.append(smoothed_val)                        # Save it\n",
    "        last = smoothed_val                                  # Anchor the last smoothed value\n",
    "        \n",
    "    return smoothed\n",
    "\n",
    "def reshape_dict(original_dict):\n",
    "    \"\"\"\n",
    "This function allows you to reshape dictionnaries by reversing their keys. \n",
    "If you have {Cell1 : {key1: data, key2: data}, Cell2 : {key1: data, key2: data}}\n",
    "you will get {key1 : {Cell1: data, Cell2: data}, key2 : {Cell1: data, Cell2: data}}\n",
    "    \"\"\"\n",
    "    reshaped_dict = {}\n",
    "\n",
    "    for cell_number, seq_dict in original_dict.items():\n",
    "        for seq_number, data_dict in seq_dict.items():\n",
    "            if seq_number not in reshaped_dict:\n",
    "                reshaped_dict[seq_number] = {}\n",
    "            reshaped_dict[seq_number][cell_number] = data_dict\n",
    "\n",
    "    return reshaped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226ad695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 00_AccCheckerboard_30Hz_20px_30sq_50%-35ND\n",
      "1 : 01_Checkerboard_30Hz_16px_40sq_50%-35ND\n",
      "2 : 02_Chirp_50Hz_50%-35ND\n",
      "3 : 03_DG_50Hz_50%-35ND\n",
      "4 : 04_Holo_DG_200Hz_50%-35D\n",
      "5 : 05_Holo_DG_200Hz_LAP4+ACET_50%-35D\n",
      "6 : 06_Flash_200Hz_LAP4+ACET_10%-50D\n",
      "7 : 07_Flash_200Hz_LAP4+ACET_10%-40D\n",
      "8 : 08_Flash_200Hz_LAP4+ACET_10%-30D\n",
      "9 : 09_Flash_200Hz_LAP4+ACET_10%-20D\n",
      "10 : 10_Flash_200Hz_LAP4+ACET_10%-10D\n",
      "11 : 11_Flash_200Hz_LAP4+ACET_10%-0D\n",
      "\n",
      "Select recording : 4\n",
      "\n",
      "Selected recording : 04_Holo_DG_200Hz_50%-35D \n",
      "\n",
      "0 : DG_Wichr_20232411_1dirs_26spots_10rep.vec\n",
      "\n",
      "Select stimulus file : 0\n",
      "\n",
      "Selected vec file : DG_Wichr_20232411_1dirs_26spots_10rep.vec\n",
      "\n",
      "Vec file length : 849720\n",
      "Total triggers number : 849720\n",
      "Triggers type loaded : visual\n",
      "Total : 297 neurons loaded \n",
      "\n",
      "Clusters id :\n",
      "[1, 2, 3, 7, 11, 14, 15, 18, 23, 25, 37, 42, 48, 53, 57, 68, 70, 71, 72, 75, 77, 80, 84, 86, 92, 97, 98, 99, 103, 107, 119, 121, 127, 131, 136, 138, 141, 142, 149, 155, 156, 160, 162, 166, 167, 170, 173, 185, 186, 201, 215, 219, 224, 226, 228, 233, 235, 236, 237, 238, 244, 248, 249, 250, 257, 259, 260, 261, 265, 266, 272, 279, 282, 285, 299, 313, 332, 334, 338, 339, 343, 348, 349, 351, 352, 363, 367, 374, 379, 381, 386, 389, 392, 393, 395, 402, 403, 408, 415, 418, 421, 424, 430, 431, 433, 439, 449, 452, 457, 459, 464, 466, 467, 468, 471, 478, 481, 482, 485, 504, 506, 507, 521, 525, 529, 530, 532, 534, 535, 536, 537, 538, 540, 544, 549, 551, 552, 553, 560, 562, 563, 564, 565, 569, 572, 580, 581, 586, 588, 599, 602, 603, 611, 615, 619, 622, 623, 624, 625, 628, 629, 631, 633, 641, 642, 650, 654, 655, 666, 667, 668, 669, 670, 674, 675, 677, 678, 682, 683, 685, 687, 688, 690, 694, 695, 700, 701, 704, 706, 709, 710, 713, 714, 718, 719, 723, 725, 726, 728, 732, 734, 739, 742, 743, 745, 746, 747, 748, 753, 754, 755, 757, 759, 760, 761, 762, 764, 769, 774, 776, 779, 780, 781, 783, 784, 786, 789, 790, 794, 796, 797, 798, 802, 803, 804, 805, 807, 810, 811, 812, 816, 817, 821, 823, 824, 827, 828, 829, 830, 831, 836, 837, 838, 840, 842, 843, 846, 847, 848, 849, 850, 852, 853, 859, 860, 862, 863, 864, 865, 866, 867, 868, 870, 872, 873, 875, 877, 879, 880, 881, 883, 884, 886, 887, 888, 891, 892, 893, 894, 895, 896, 897, 899, 900, 901, 902, 903]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Variables\n",
    "    \n",
    "    DO NOT CHANGE VALUES HERE UNLESS DEBUG/SPECIFIC USE\n",
    "    \n",
    "    All the variables used in this part of the cell should always refere to your 'params.py' file\n",
    "    unless you want to manually change them only for this run (i.e. debugging). \n",
    "    You may have to add those variable into the function you want to adapt as only the minimal \n",
    "    amount of var are currently given to functions as inputs.\n",
    "\"\"\"\n",
    "\n",
    "recording_names = params.recording_names\n",
    "\n",
    "#Experiment name\n",
    "exp = params.exp\n",
    "\n",
    "#Analysis output directory\n",
    "output_directory=params.output_directory\n",
    "\n",
    "#Sampling rate of the mea\n",
    "fs = params.fs  \n",
    "\n",
    "#Trigger directory\n",
    "triggers_directory= params.triggers_directory\n",
    "\n",
    "#Vec files drectory\n",
    "vec_directory = os.path.join(params.root,'VEC_Files')\n",
    "\n",
    "#List all vec files in vec files directory\n",
    "available_vec = os.listdir(os.path.normpath(vec_directory))\n",
    "\n",
    "\"\"\"\n",
    "    Input\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#Find rec triggers\n",
    "print(*['{} : {}'.format(i,recording_name) for i, recording_name in enumerate(recording_names)], sep=\"\\n\")\n",
    "recording_number = int(input(\"\\nSelect recording : \"))\n",
    "rec = recording_names[recording_number]\n",
    "print(f\"\\nSelected recording : {rec} \\n\")\n",
    "\n",
    "analysis_directory = os.path.normpath(os.path.join(output_directory,r'Vec_Analysis_rec_{}'.format(recording_number)))\n",
    "if not os.path.isdir(analysis_directory): os.makedirs(analysis_directory)\n",
    "\n",
    "#Find stim vec file\n",
    "print(*['{} : {}'.format(i,vec_file) for i, vec_file in enumerate(available_vec)], sep=\"\\n\")\n",
    "vec_number = int(input(\"\\nSelect stimulus file : \"))\n",
    "\n",
    "\"\"\"\n",
    "    Processing\n",
    "\"\"\"\n",
    "\n",
    "#Load vec file\n",
    "vec = np.loadtxt(os.path.join(vec_directory,available_vec[vec_number]))[1:,:]  #Remove first line as it is not a trigger\n",
    "print(f\"\\nSelected vec file : {available_vec[vec_number]}\\n\")\n",
    "print(f\"Vec file length : {vec.shape[0]}\")\n",
    "\n",
    "#load triggers\n",
    "trig_data = load_obj(os.path.normpath(os.path.join(triggers_directory,'{}_{}_triggers.pkl'.format(exp,rec))))\n",
    "trig_indices = trig_data['indices']\n",
    "stim_onsets = trig_data['indices']/fs\n",
    "print(f\"Total triggers number : {len(stim_onsets)}\")\n",
    "print(f\"Triggers type loaded : {trig_data['trigger_type']}\")\n",
    "\n",
    "spike_trains=load_obj(os.path.join(output_directory, r'{}_fullexp_neurons_data.pkl'.format(exp)))\n",
    "\n",
    "cells=list(spike_trains.keys())\n",
    "spike_times={}\n",
    "for cell in cells:\n",
    "    spike_times[cell] = (spike_trains[cell][rec])\n",
    "    \n",
    "print('Total : {} neurons loaded \\n\\nClusters id :\\n{}\\n'.format(len(spike_trains.keys()),cells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73160ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigers already extracted previously. Write again files files? Type Yes to do so :\n",
      "y\n",
      "Splitting spikes into sequences for each cluster...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485fc9562c0342b89922ac404dc9b6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "----- Done -----\n"
     ]
    }
   ],
   "source": [
    "dict_name = f'{rec}_dict.pkl'\n",
    "file_already_exists = os.path.exists(os.path.join(analysis_directory, dict_name))\n",
    "n_bin=40  #binning for the psth, default = 40, if n_bin = \"relative\", n_bin = sqrt(nb of spikes in the raster)\n",
    "n_bin ='relative'\n",
    "\n",
    "if not file_already_exists or (str(input('Trigers already extracted previously. Write again files files? Type Yes to do so :\\n')) in [\"Y\", \"y\", \"yes\", \"Yes\"]):\n",
    "    print(\"Splitting spikes into sequences for each cluster...\")\n",
    "    rec_dict = {}\n",
    "    for i in tqdm(cells):\n",
    "\n",
    "        #Process\n",
    "        rec_dict[i]={}\n",
    "        trig_seq  = get_sequences_triggers(stim_onsets, vec)\n",
    "        spike_seq = get_spikes_sequences(spike_times[i], trig_seq)\n",
    "        raster    = spikeseq2raster(spike_seq, trig_seq)\n",
    "        psth      = spikeseq2psth(raster, trig_seq, n_bin=n_bin)\n",
    "        \n",
    "        #Save in dict\n",
    "        for key in raster.keys():\n",
    "            if key=='':\n",
    "                continue\n",
    "            rec_dict[i][key]={}\n",
    "            rec_dict[i][key]['raster']   = raster[key]\n",
    "            rec_dict[i][key]['psth']     = psth[key]\n",
    "\n",
    "            rec_dict[i][key]['triggers'] = {\n",
    "                'start':trig_seq['1'+key+'00'][0], \n",
    "                'end':trig_seq['1'+key+'00'][-1], \n",
    "                'rng':(0, trig_seq['1'+key+'00'][-1]-trig_seq['1'+key+'00'][0] + np.mean(np.diff(trig_seq['1'+key+'00'])))\n",
    "                }\n",
    "        rec_dict[i]['trig_seq']  = trig_seq\n",
    "        rec_dict[i]['spike_seq'] = spike_seq\n",
    "    print(\"Saving...\")\n",
    "    save_obj(rec_dict, os.path.join(analysis_directory, dict_name))\n",
    "else:\n",
    "    print(f\"Dictionnary loaded from : \\n{os.path.join(analysis_directory, dict_name)}\")\n",
    "    rec_dict = load_obj(os.path.join(analysis_directory, dict_name))\n",
    "    \n",
    "    \n",
    "print(\"----- Done -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5193783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2714be8998914422b48c1bf1f4793350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/158 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clusters_as_folder = False\n",
    "color =\"#B85A8F\"\n",
    "\n",
    "\n",
    "if clusters_as_folder:\n",
    "    dict_to_plot = rec_dict\n",
    "    element = \"Cell\"\n",
    "    scd_element = \"Sequence\"\n",
    "\n",
    "else:\n",
    "    dict_to_plot = reshape_dict(rec_dict)\n",
    "    element = \"Sequence\"\n",
    "    scd_element = \"Cell\"\n",
    "\n",
    "for elt in tqdm(dict_to_plot.keys()):\n",
    "    item_directory = os.path.normpath(os.path.join(analysis_directory,f'{element}_{elt}'))\n",
    "    if not os.path.isdir(item_directory): os.makedirs(item_directory)\n",
    "    \n",
    "    for scd_elt in dict_to_plot[elt].keys():\n",
    "        \n",
    "        #Plot all the rasters. Takes a few seconds.\n",
    "        fig, axs = plt.subplots(nrows = 2,ncols = 1, sharex=True, gridspec_kw={'height_ratios': [3, 1]}, figsize=(10,10))\n",
    "        ax_rast = axs[0]\n",
    "        ax_rast.eventplot(dict_to_plot[elt][scd_elt][\"raster\"], color=color)\n",
    "        ax_rast.set(title = \"Raster plot\", ylabel='N Repetitions')\n",
    "\n",
    "        ax_psth = axs[1]\n",
    "        y    = dict_to_plot[elt][scd_elt][\"psth\"]\n",
    "        rng  = dict_to_plot[elt][scd_elt]['triggers']['rng']\n",
    "        y    = y*(len(y)/(rng[1]-rng[0]))   #Turning number of spikes into firing rate \n",
    "        \n",
    "        ysmooth = smooth(y,.4)\n",
    "        \n",
    "        x=np.linspace(rng[0],rng[1],len(y))\n",
    "        ax_psth.fill_between(x, ysmooth,0,alpha=1, color=color)\n",
    "        ax_psth.set(xlabel='Time in sec', ylabel='Firing rate (spikes/s)')\n",
    "        ax_psth.set_ylim(bottom=-0.1, top=max(1, max(ysmooth)))\n",
    "\n",
    "        plt.suptitle(f'{scd_element}_{scd_elt}')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "#         plt.show(block=False)\n",
    "        plt.savefig(os.path.join(item_directory,f'{scd_element}_{scd_elt}.png'))\n",
    "        plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
