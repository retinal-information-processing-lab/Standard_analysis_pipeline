{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import params\n",
    "from utils import *\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import math\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# For clustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy as sc\n",
    "from sklearn.decomposition import SparsePCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Load triggers and spikes + select chirp type !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = params.exp\n",
    "\n",
    "#choose the chirp recording\n",
    "rec=\"02_Chirp_20reps_25ND50%_50Hz.raw\"\n",
    "\n",
    "#If False: new chirp at 50Hz, if True: old 2p room chirp   !!!!!!!!!\n",
    "old=False\n",
    "\n",
    "#load triggers\n",
    "trig_data = load_obj(os.path.normpath(os.path.join(params.triggers_directory,'{}_{}_triggers.pkl'.format(exp,rec))))\n",
    "stim_onsets = trig_data['indices']/params.fs   \n",
    "\n",
    "#load spikes\n",
    "output_directory=params.output_directory\n",
    "\n",
    "spike_trains=load_obj(os.path.join(output_directory, r'{}_fullexp_neurons_data.pkl'.format(exp)))\n",
    "\n",
    "cells=list(spike_trains.keys())\n",
    "spike_times=[]\n",
    "for cell in cells:\n",
    "    spike_times.append(spike_trains[cell][rec])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Chirp rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if old: \n",
    "    nb_repetitions = 30\n",
    "    n_bins=625\n",
    "    rep_lenght=25\n",
    "else: \n",
    "    nb_repetitions = 20 \n",
    "    n_bins=800             #CHANGE HERE THE NUMBER OF BINS IF NECESSARY\n",
    "    rep_lenght=32\n",
    "time_bin= rep_lenght/n_bins #in seconds\n",
    "\n",
    "cell_data={}\n",
    "for idx,cell_nb in tqdm(enumerate(cells[:])):\n",
    "    if not cell_nb in cell_data.keys(): cell_data[cell_nb]={}\n",
    "    \n",
    "    #Get spike_times\n",
    "    euler_sptimes = spike_times[idx]\n",
    "\n",
    "    aligned_triggers = stim_onsets  #(in seconds)\n",
    "    \n",
    "    # Flashes: Get the repeated sequence times for the specified position\n",
    "    repeated_sequences_times = []\n",
    "    for i in range(0,nb_repetitions):\n",
    "        if old: times = aligned_triggers[i*999:999*(i+1)]\n",
    "        else: times = aligned_triggers[i*1600+151:151+1600*(i+1)]\n",
    "        repeated_sequences_times += [[times[0], times[-1]]]\n",
    "\n",
    "    # Build the spike trains corresponding to stimulus repetitions\n",
    "    spike_trains = []\n",
    "    for i in range(len(repeated_sequences_times)):\n",
    "        spike_train = restrict_array(euler_sptimes, repeated_sequences_times[i][0], repeated_sequences_times[i][1])\n",
    "        spike_trains += [spike_train]\n",
    "\n",
    "    # Align the spike trains\n",
    "    for i in range(len(spike_trains)):\n",
    "        spike_trains[i] = spike_trains[i] - repeated_sequences_times[i][0]\n",
    "        \n",
    "    # Compute psth\n",
    "    binned_spikes = np.empty((nb_repetitions,n_bins))   #40 ms time bin\n",
    "    for i in range(nb_repetitions):\n",
    "       \n",
    "       binned_spikes[i,:] = np.histogram(spike_trains[i], bins=n_bins, range=(0,rep_lenght))[0]\n",
    "    binned_spikes = np.sum(binned_spikes, axis=0)\n",
    "    \n",
    "    # Transform spike count in firing rate\n",
    "    binned_spikes = binned_spikes / time_bin\n",
    "\n",
    "    cell_data[cell_nb][\"spike_times\"] = spike_times\n",
    "    cell_data[cell_nb][\"repeated_sequences_times\"] = repeated_sequences_times\n",
    "    cell_data[cell_nb][\"spike_trains\"] = spike_trains\n",
    "    cell_data[cell_nb][\"psth\"] = binned_spikes    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Plot Chirp rasters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With addaed spatial STA in order to be able to use this plot alone to do cell selection for clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center><i>REQUIRES CELL 2 RUN AND CHECKERBOARD ANALYSIS</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rasters\n",
    "\n",
    "fig_directory = os.path.normpath(os.path.join(output_directory,r'Chirp_rasters+STA'))\n",
    "if not os.path.isdir(fig_directory): os.makedirs(fig_directory)\n",
    "\n",
    "sta_results=np.load(os.path.join(output_directory,'sta_data_3D_fitted.pkl'),allow_pickle=True)    \n",
    "\n",
    "if old:\n",
    "    vec_path = os.path.join('./types_matching', r\"EulerStim180530.vec\")\n",
    "    euler_vec = -np.genfromtxt(vec_path)\n",
    "    rep_lenght = 25\n",
    "    n_bins = 625\n",
    "    n_reps = 30\n",
    "    \n",
    "else:    \n",
    "    vec_path = os.path.join('./types_matching', r\"Euler_50Hz_20reps_1024x768pix.vec\")\n",
    "    euler_vec = np.genfromtxt(vec_path) \n",
    "    rep_lenght = 32\n",
    "    n_bins = 800\n",
    "    n_reps = 20\n",
    "\n",
    "time_bin = rep_lenght/n_bins #in seconds\n",
    "for cell_nb in tqdm(cells[:]):\n",
    "\n",
    "    fig = plt.figure(figsize=(19,6))\n",
    "    gs = GridSpec(8, 19, left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.4, hspace=0, figure=fig)\n",
    "#     fig=plt.figure(figsize=(16,6))\n",
    "\n",
    "    #plot Chirp stimulus\n",
    "    ax=fig.add_subplot(gs[0:2, :-3])\n",
    "    if old:ax.plot(np.linspace(0,rep_lenght,999),euler_vec[1:1000,1], color='k')\n",
    "    else: ax.plot(np.linspace(0,rep_lenght,1600),euler_vec[151:1751,1], color='k')\n",
    "    ax.set_ylabel('Chirp Stimulus')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('Cluster {}'.format(cell_nb))\n",
    "    ax.set_xlim([0,rep_lenght])\n",
    "\n",
    "    #plot chirp raster\n",
    "    ax=fig.add_subplot(gs[2:5, :-3])\n",
    "    ax.eventplot(cell_data[cell_nb][\"spike_trains\"],color='k',lw=1,linelengths=1)\n",
    "    ax.set_ylabel('#Trial')\n",
    "    ax.set_xlim([0,rep_lenght])\n",
    "    \n",
    "    #plot chirp psth\n",
    "    ax=fig.add_subplot(gs[5:, :-3])\n",
    "    ax.step(np.linspace(0,rep_lenght,n_bins),cell_data[cell_nb][\"psth\"])\n",
    "    ax.set_ylabel('Firing rate Hz \\n ({} ms time bin)'.format(int(time_bin*1000)))\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_xlim([0,rep_lenght])\n",
    "    \n",
    "    #plot spatial STA\n",
    "    ax = fig.add_subplot(gs[0:3, -3:])\n",
    "    ax.set_title(\"STA\",fontsize=12 )\n",
    "    spatial = sta_results[cell_nb]['center_analyse']['Spatial']\n",
    "    spatial = spatial**2*np.sign(spatial)\n",
    "    cmap='RdBu_r'\n",
    "    image = ax.imshow(spatial, cmap=cmap,interpolation='gaussian')\n",
    "    abs_max = 0.5*max(np.max(spatial), abs(np.min(spatial)))\n",
    "    image.set_clim(-abs_max,abs_max)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    fsave = os.path.join(fig_directory, '{}_Chirp_raster+STA'.format(cell_nb)) \n",
    "    fig.savefig(fsave+'.png',format='png',dpi=90)\n",
    "    plt.close(fig)\n",
    "    \n",
    "print('Done!')    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Select the cells suted for clustering and save them\n",
    "Those that have a nice response to the chirp and a well defined STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selection_already_done=False\n",
    "\n",
    "if selection_already_done:\n",
    "    selected_cells=load_obj(os.path.join(output_directory, '{}_selected_cells_for_clustering.pkl'.format(exp) ))\n",
    "\n",
    "else:\n",
    "    bad_STAs = [29,52,60,67,140,154,228,248,271,300,365,545,609,624,668,683,764,802,908,937,1026,1213,1239,1248,1298,1385,]\n",
    "    bad_chirps = [353,452,474,480,483,707,719,732,826,831,900,906,918,960,1017,1019,1091,1109,1124,1149,1156,1162,\n",
    "                 1168,1249,1260,1277,1288,1307,1315,1323,1324,1336,]                                                                      \n",
    "    bad_chirps2 =[8,11,20,81,133, 163,173,216,258,270,283,284,306,310,325,329,330,385,422,459,467,539,565,580,628,\n",
    "                 659,661,670,686,723,763,820,898,915,931,991,1063,1067,1099,1170,1208,1221,1222,1227,1247,1280,1299,\n",
    "                 1325,1338,1362,1365,]  # not many spikes2\n",
    "\n",
    "    selected_cells = []\n",
    "    for c in cells:\n",
    "        if not c in bad_STAs and not c in bad_chirps and not c in bad_chirps2:\n",
    "            selected_cells+= [c]\n",
    "\n",
    "    print(\"Selected {} cells.\".format(len(selected_cells)))\n",
    "    len(cells),len(selected_cells)\n",
    "\n",
    "    #and save the selected list\n",
    "    fsave = os.path.join(output_directory, '{}_selected_cells_for_clustering'.format(exp) )  \n",
    "    save_obj(selected_cells,fsave)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Import the sta results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_results=np.load(os.path.join(output_directory,'sta_data_3D_fitted.pkl'),allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Cell typing: Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-you want to move the distance threshold until you have roughly 50 clusters\n",
    "-you want to have a number of PCs that cumulatively can explain around 80% of the variance in the dataset\n",
    "-you can decide how many components of the STA to choose in the clustering (usually 2 if the checkerboard recording is reliable and 1 otherwise)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center><i>REQUIRES CELL 2, CELL 4 AND CELL 5 RUN </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sparse=False\n",
    "# change dist_thres to adapt the cut of the dendrogram and select the number of clusters\n",
    "#####################################################################\n",
    "dist_thres = 35#13\n",
    "#####################################################################\n",
    "\n",
    "n_cells = len(selected_cells)\n",
    "#-----------------------------------\n",
    "#-----------------------------------\n",
    "# Get Euler PCA\n",
    "n_rep = 20 # nb of repeats\n",
    "nt = 32 # total length\n",
    "dt = 0.04 # bin size in seconds\n",
    "time_bins = np.arange(0,nt+dt,dt)\n",
    "\n",
    "spikes = np.zeros((n_cells, int(nt/dt), n_rep))\n",
    "for cell_index in range(len(selected_cells)):\n",
    "    cell_id = selected_cells[cell_index]\n",
    "    spike_cell = cell_data[cell_id][\"spike_trains\"]\n",
    "    for rep in range(n_rep):\n",
    "        temp=np.histogram(spike_cell[rep], bins=time_bins)\n",
    "        spikes[cell_index,:,rep] = temp[0]\n",
    "\n",
    "#-------------------------\n",
    "# Pre process the PSTH\n",
    "psth = np.mean(spikes, 2)\n",
    "psth_z = sc.stats.zscore(psth, 1)\n",
    "\n",
    "# Select number of PCs to keep so to explain ~80% of the variance\n",
    "#####################################################################\n",
    "n_components_psth = 95  #13#16\n",
    "#####################################################################\n",
    "if sparse:\n",
    "    pca_transformer = SparsePCA(n_components_psth, random_state=0).fit(psth_z)\n",
    "else:\n",
    "    pca_transformer = PCA(n_components_psth).fit(psth_z)\n",
    "psth_pca = pca_transformer.transform(psth_z)\n",
    "# -----------------------------------\n",
    "# -----------------------------------\n",
    "# Get checkerboard STA PCA\n",
    "STA_time_course = np.zeros((n_cells,21))  #21 data points for these STAs\n",
    "for cell_index in range(len(selected_cells)):\n",
    "    cell_id = selected_cells[cell_index]\n",
    "    TempSTA_cell = sta_results[selected_cells[cell_index]]['center_analyse']['Temporal'][-21:]\n",
    "    STA_time_course[cell_index] = TempSTA_cell\n",
    "#---------------------------\n",
    "# Pre process the STA\n",
    "sta_tc = sc.stats.zscore(STA_time_course[:,:], 1)\n",
    "#####################################################################\n",
    "n_components_sta_tc = 1\n",
    "#####################################################################\n",
    "if n_components_sta_tc>0:\n",
    "    pca_transformer2 = PCA(n_components_sta_tc).fit(sta_tc)\n",
    "    sta_tc_pca = pca_transformer2.transform(sta_tc)\n",
    "#-----------------------------------\n",
    "#-----------------------------------\n",
    "if not sparse:\n",
    "    plt.plot(np.arange(n_components_psth)+1, np.cumsum(pca_transformer.explained_variance_ratio_)*100)\n",
    "    plt.axhline(y=80, color='k')\n",
    "    plt.xlabel('number of PCs')\n",
    "    plt.ylabel('% of cumulative explained variance')\n",
    "    plt.show()\n",
    "\n",
    "    if n_components_sta_tc>0:\n",
    "        plt.plot(np.arange(n_components_sta_tc)+1, np.cumsum(pca_transformer2.explained_variance_ratio_)*100,'o-')\n",
    "        plt.axhline(y=80, color='k')\n",
    "        plt.xlabel('number of PCs')\n",
    "        plt.ylabel('% of cumulative explained variance')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "#-----------------------------------\n",
    "#-----------------------------------\n",
    "cluster_dataset = np.zeros((n_cells, n_components_psth+n_components_sta_tc+1))\n",
    "cluster_dataset[:,:n_components_psth] = psth_pca\n",
    "if n_components_sta_tc>0:\n",
    "    cluster_dataset[:,n_components_psth:n_components_psth+n_components_sta_tc] = sta_tc_pca\n",
    "\n",
    "ell_size=np.zeros(len(selected_cells))\n",
    "for cell_index in range(len(selected_cells)):\n",
    "    cell_id = selected_cells[cell_index]\n",
    "    width,height = [sta_results[selected_cells[cell_index]]['center_analyse']['EllipseCoor'][3],  sta_results[selected_cells[cell_index]]['center_analyse']['EllipseCoor'][4]]\n",
    "#     width,height = cell_data[cell_id][\"ellipseSigmaXY\"]\n",
    "    ell_size[cell_index] = np.abs(np.pi*width*height)\n",
    "    \n",
    "ell_size_temp = -np.ones(n_cells)\n",
    "temp = ell_size[:]-ell_size[:].min()\n",
    "ell_size_temp[:] = temp/temp.max()\n",
    "cluster_dataset[:,-1] = ell_size_temp\n",
    "#-----------------------------------\n",
    "#-----------------------------------\n",
    "\n",
    "# perform agglomerative clustering\n",
    "model = AgglomerativeClustering(distance_threshold=dist_thres, n_clusters=None)\n",
    "#model = model.fit(psth_pca)\n",
    "model = model.fit(cluster_dataset)\n",
    "\n",
    "# plot the dendrogram\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plot_dendrogram(model, truncate_mode='level', p=0)\n",
    "plt.axhline(dist_thres, color='k')\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "\n",
    "# plot the cluster centroids\n",
    "n_clusts = len(np.unique(model.labels_))\n",
    "plt.figure()\n",
    "for iclust in range(n_clusts):\n",
    "    idx_cluster = np.where(model.labels_==iclust)[0]\n",
    "    plt.plot(np.mean(psth_z[idx_cluster,:], 0) + iclust * 5)\n",
    "plt.show() \n",
    "    \n",
    "print('Number of clusters: ',len(np.unique(model.labels_)))\n",
    "\n",
    "# plot the psths of all cells in one cluster\n",
    "for icluster in range(len(np.unique(model.labels_))):\n",
    "    # icluster = 0\n",
    "    idx_cluster = np.where(model.labels_==icluster)[0]\n",
    "    print(f'cluster size : {len(idx_cluster)}')\n",
    "    plt.figure()\n",
    "    plt.plot(psth_z[idx_cluster,:].transpose())\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Save each cell's cluster number in the chirp response dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cluster of each cell\n",
    "for cell_index in range(len(selected_cells)):\n",
    "    cell_nb = selected_cells[cell_index]\n",
    "    cell_data[cell_nb][\"type\"] = model.labels_[cell_index]\n",
    "for cell in cells:\n",
    "    if cell not in selected_cells:\n",
    "        cell_data[cell][\"type\"] = 'Not assigned'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Create a summary figure for each cluster type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center><i>REQUIRES CELL 6 RUN </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig_directory = os.path.normpath(os.path.join(output_directory,r'Cell_typing'))\n",
    "if not os.path.isdir(fig_directory): os.makedirs(fig_directory)\n",
    "\n",
    "if old:\n",
    "    vec_path = os.path.join('./types_matching', r\"EulerStim180530.vec\")\n",
    "    euler_vec = -np.genfromtxt(vec_path)  \n",
    "else:    \n",
    "    vec_path = os.path.join('./types_matching', r\"Euler_50Hz_20reps_1024x768pix.vec\")\n",
    "    euler_vec = np.genfromtxt(vec_path) \n",
    "\n",
    "\n",
    "for icluster in range(len(np.unique(model.labels_)))[:]:\n",
    "    idx_cluster = np.where(model.labels_==icluster)[0]\n",
    "    print('Number of cells in cluster {}: {}'.format(icluster,len(idx_cluster)))\n",
    "\n",
    "    gs = GridSpec(len(idx_cluster)+2,6)\n",
    "    \n",
    "    if len(idx_cluster)<7:\n",
    "        yspan = 2\n",
    "    else:\n",
    "        yspan=-2\n",
    "    fig=plt.figure(figsize=(14,(len(idx_cluster)+yspan)*1.75))\n",
    "    plt.suptitle(\"Cell group {}.\\n {} cells.\".format(icluster, len(idx_cluster)))\n",
    "\n",
    "    #-------------------------------\n",
    "    # Loop cells in cluster\n",
    "    line = 2\n",
    "    STAs=np.zeros(21)\n",
    "    STAcount=0\n",
    "    waves=np.zeros(101)\n",
    "    wavecount=0\n",
    "    ax0= fig.add_subplot(gs[0:2,0:2])\n",
    "    for index in sorted(idx_cluster):\n",
    "        cell_nb = selected_cells[index]\n",
    "        \n",
    "        #-----------------\n",
    "        # Plot temp STA\n",
    "        ax= fig.add_subplot(gs[line,1])\n",
    "        \n",
    "#         ax.set_ylim([-4,4])\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_aspect(0.175)\n",
    "        temporal_sta=sta_results[cell_nb]['center_analyse']['Temporal'][-21:]\n",
    "        ax.step(np.linspace(-21/30,0,21),temporal_sta,'k',lw=3)\n",
    "#         ax.set_title('Cluster {}'.format(cell_nb))\n",
    "        ax.axhline(0,color='k',lw=0.5)\n",
    "        \n",
    "        STAs+=temporal_sta\n",
    "        STAcount+=1\n",
    "        \n",
    "        #-----------------\n",
    "        # Plot temp STA avg\n",
    "        ax= fig.add_subplot(gs[0,2])\n",
    "        \n",
    "        ax.set_title('Temp STA')\n",
    "        ax.set_ylim([-4,4])\n",
    "        ax.plot(np.linspace(-21/30,0,21),temporal_sta,lw=0.5)\n",
    "        ax.axhline(0,color='k',lw=0.5)\n",
    "        ax.set_xlabel('Time(s)')\n",
    "        \n",
    "         #-----------------\n",
    "        # plot Chirp\n",
    "        ax= fig.add_subplot(gs[line,3:])\n",
    "        \n",
    "        cell_index=selected_cells.index(cell_nb)\n",
    "        ax.plot(np.linspace(0,32,800),cell_data[cell_nb][\"psth\"])\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_xticks([])\n",
    "        plt.locator_params(axis=\"y\", nbins=3)\n",
    "        \n",
    "        \n",
    "        #-----------------\n",
    "        # plot Spatial STA\n",
    "        ax= fig.add_subplot(gs[line,0])\n",
    "        \n",
    "        parameters=sta_results[cell_nb]['center_analyse']['EllipseCoor']\n",
    "        x0=parameters[1]\n",
    "        y0=parameters[2]\n",
    "        \n",
    "        ax = plot_sta(ax, sta_results[cell_nb]['center_analyse']['Spatial'],parameters)\n",
    "        ax.set_xlim(x0-4,x0+4)\n",
    "        ax.set_ylim(y0+4,y0-4)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        gaussian = gaussian2D(sta_results[cell_nb]['center_analyse']['Spatial'].shape,*parameters)\n",
    "        if parameters[0] != 0:\n",
    "            ax0.contour(np.abs(gaussian),levels = [0.6*np.max(np.abs(gaussian))], colors='k',linestyles = 'solid', alpha = 0.8)\n",
    "\n",
    "        ax= fig.add_subplot(gs[line,2])\n",
    "        ax.annotate('Cluster {}'.format(cell_nb), (0,0.5), (0, 0.5), fontsize=15)\n",
    "\n",
    "        ax.axis('off')\n",
    "        \n",
    "        line+=1\n",
    "    #-----------------\n",
    "    # avg STA\n",
    "    STAs=STAs/STAcount\n",
    "    ax= fig.add_subplot(gs[0,2])\n",
    "    ax.plot(np.linspace(-21/30,0,21),STAs,'k',lw=2)\n",
    "#     ax.set_ylim([-4,4])\n",
    "    ax.set_aspect(0.175)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    #-----------------\n",
    "    #size ellipses\n",
    "    \n",
    "    ax0.set_title('Ellipses')\n",
    "#     ax0.set_xlim(4,20)\n",
    "#     ax0.set_ylim(20,4)\n",
    "    ax0.set_aspect('equal')\n",
    "    ax0.set_xticks([])\n",
    "    ax0.set_yticks([])\n",
    "        \n",
    "    #-----------------\n",
    "    # mean chirp psth\n",
    "    ax= fig.add_subplot(gs[0,3:])\n",
    "    ax.set_title('Chirp psth')\n",
    "    \n",
    "    ax.plot(np.linspace(0,32,800),np.mean(psth_z[idx_cluster,:], 0),'b')\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    #-----------------\n",
    "    # plot chirp stim\n",
    "    ax= fig.add_subplot(gs[1,3:])\n",
    "    \n",
    "    ax.plot(np.linspace(0,32,1600),euler_vec[0+151:151+1600,1]*1., color='k')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylim([-100,350])\n",
    "    ax.set_xlabel('Time(s)')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(True)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    \n",
    "    fsave = os.path.join(fig_directory, 'Cluster_{}'.format(icluster))\n",
    "    fig.savefig(fsave+'.png',format='png',dpi=250)\n",
    "    plt.close(fig)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: When satisfied with the clustering, save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsave = os.path.join(output_directory, '{}_cell_typing_data'.format(exp) )  \n",
    "save_obj(cell_data,fsave)\n",
    "\n",
    "# fsave = os.path.join(output_directory, 'Exp{}_clustermodel'.format(exp) )  \n",
    "# save_obj([model,psth_z],fsave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------work in progress-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcium_exp(x):\n",
    "    a =           0\n",
    "    b =       1.086 \n",
    "    c =       1.747 -1.0              ##################### changes here\n",
    "    val = a+b*np.exp(-c*x)\n",
    "    return val\n",
    "\n",
    "def toCalciumLinear(time_sequence, spike_train):\n",
    "    calcium_filter = calcium_exp(time_sequence);\n",
    "    \n",
    "    stitch3 = np.append(spike_train,[spike_train,spike_train])\n",
    "    \n",
    "    calcium_trace = np.convolve(stitch3, calcium_filter, 'full')[len(spike_train):len(spike_train)*2];\n",
    "    calcium_trace = calcium_trace-min(calcium_trace)\n",
    "    calcium_trace=calcium_trace/max(calcium_trace)\n",
    "    return calcium_trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching with Baden et al. clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_matching_folder= './types_matching'\n",
    "baden=loadmat(os.path.join( types_matching_folder,'baden_data.mat'))\n",
    "calcium=loadmat(os.path.join( types_matching_folder,'calcium_conversion.mat'))\n",
    "\n",
    "#the normalized profile of the chirp stimulus used in Baden et al.  #31988 points\n",
    "chirp_stim=baden['chirp_stim'][:,0]/max(baden['chirp_stim'][:,0])\n",
    "\n",
    "#the triggers used in Baden et al. plus two seconds (to match them with our stimulus)  #31988 points\n",
    "chirp_stim_time = baden['chirp_stim_time'][0,:] +2\n",
    "\n",
    "# a 249 points array from 0 to 32 seconds. Times of calcium sampling at 8Hz?\n",
    "baden_time_original = baden['chirp_time'][0,:]\n",
    "\n",
    "# a (11210, 1) array. I suppose 11210 were the measurements taken and in this array are the labels of the groups\n",
    "# in which each measurment was clustered\n",
    "group_idx = baden['group_idx'] \n",
    "\n",
    "# the average calcium traces for each measurement. Each measurement has 249 time points.\n",
    "psth_euler = baden['chirp_avg']\n",
    "\n",
    "#here I load from a csv the labels of the Baden types\n",
    "euler_labels_f = open(os.path.join( types_matching_folder, 'Baden Types'))\n",
    "euler_labels_f = csv.reader(euler_labels_f,delimiter=\",\");\n",
    "euler_labels={}    #this contains the 32 labels of the euler cell types\n",
    "c=0\n",
    "for row in euler_labels_f:\n",
    "    if c==0:\n",
    "        stim_cond_head = row\n",
    "        c=1\n",
    "    else:\n",
    "        euler_labels[c-1]= row\n",
    "        c+=1\n",
    "#------------------- \n",
    "\n",
    "n_baden_types = len(euler_labels)\n",
    "\n",
    "#our stimulus. Load it and normalize it\n",
    "vec_path = './types_matching/Euler_50Hz_20reps_1024x768pix.vec'\n",
    "euler_vec = np.genfromtxt(vec_path)\n",
    "euler_vec = euler_vec[151:151+1600,1]/max(euler_vec[151:151+1600,1])\n",
    "\n",
    "#### Repeat this check for the older chirp version!!!!!!\n",
    "\n",
    "nb_chirp_bins=cell_data[cells[0]][\"psth\"].size\n",
    "\n",
    "plt.figure(figsize = (16,4))\n",
    "time_stim = np.linspace(0,32,1600+1)[:-1]\n",
    "time = np.linspace(0,32,nb_chirp_bins+1)[:-1]\n",
    "# Our chirp\n",
    "plt.plot(time_stim, euler_vec, label='our chirp')\n",
    "euler_vec.shape\n",
    "euler_vec[::5].shape,time.shape\n",
    "# Baden chirp\n",
    "plt.plot(chirp_stim_time ,chirp_stim+1.5, label = 'Baden chirp')\n",
    "chirp_stim_time.shape, chirp_stim.shape\n",
    "plt.legend(frameon=False)\n",
    "\n",
    "# Comparison range:\n",
    "comp_range = [0.1,31.9]         # to avoid prestep effects ### why 0.1 and not 2 ?\n",
    "delta_range = comp_range[1]-comp_range[0]\n",
    "time_common = np.linspace(comp_range[0],comp_range[1],int(delta_range/0.05) +2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make ours and the baden traces match we have to shift Baden's sampling times of two seconds. But then this will create missing sampling times for Baden and so here we take the last two seconds worth of Baden's sampling times and we stitch them at the beginning of the sampling times sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two additional seconds at the end of Baden stim that will be stitched in the beginning\n",
    "baden_time = baden_time_original+2\n",
    "baden_first = baden_time[-15:]-32 \n",
    "baden_time = np.append(baden_first,baden_time[:-15])\n",
    "\n",
    "#we do the same for the calcium traces\n",
    "Baden_types = []\n",
    "\n",
    "plt.figure(figsize=(8,16))\n",
    "\n",
    "for i in np.arange(n_baden_types)+1:\n",
    "    #here the calcium traces of all the recordings of each type are averaged together\n",
    "    trace = np.mean(psth_euler[:,(group_idx==i)[:,0]],1)  \n",
    "    #and here they are all put between 0 and 1\n",
    "    trace = trace-min(trace)\n",
    "    trace = trace/max(trace)\n",
    "    trace = np.append(trace[-15:],trace[:-15])                # I stitch the last 2 s in the beggining so that it is \n",
    "                                                              #  the same as my experimental stim\n",
    "    Baden_types.append(trace)\n",
    "    plt.plot(baden_time,trace-i)\n",
    "    plt.text(35,-i,str(i)+'  '+euler_labels[i-1][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the clustering results into calcium traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make it loadable!!\n",
    "\n",
    "labels = model.labels_\n",
    "exp_labels = np.sort(np.unique(labels))\n",
    "psth_z.shape, 'Ncells  - Ndatapoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate experiment type traces\n",
    "Exp_types = []\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1,2,1)\n",
    "for i in exp_labels:\n",
    "    #I repeat to my spiking data the treatment I did for the Euler's calcium traces\n",
    "    trace = np.mean(psth_z[(labels==i),:],0)\n",
    "    trace = trace-min(trace)\n",
    "    trace = trace/max(trace)\n",
    "    Exp_types.append(trace)\n",
    "    plt.plot(time,trace+i)\n",
    "    plt.title('Mean psth')\n",
    "\n",
    "# Experiment to calcium\n",
    "Exp_types_Ca = {}\n",
    "fig.add_subplot(1,2,2)\n",
    "for i in exp_labels:\n",
    "    # I transform each cluster's mean psth into a calcium trace\n",
    "    trace = toCalciumLinear(time,Exp_types[i])\n",
    "    Exp_types_Ca[i] = trace    #the only difference is that my made-up calcium traces have the dimension of the \n",
    "                                #number of bins the PSTHs had while the baden traces have dimension 249\n",
    "    plt.plot(time,trace+i)\n",
    "    plt.title('Mean Calcium transform')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlate Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "# Interpolate both calcium traces (Baden and transformed traces) and then calculate them on the same set of times\n",
    "Baden_common = {}\n",
    "for i in np.arange(n_baden_types):\n",
    "    f = interpolate.interp1d(baden_time, Baden_types[i])\n",
    "    Baden_common[i] = f(time_common)   # use interpolation function returned by `interp1d`\n",
    "\n",
    "Exp_common={}\n",
    "for i in exp_labels:\n",
    "    f = interpolate.interp1d(time, Exp_types_Ca[i])\n",
    "    Exp_common[i] = f(time_common)   # use interpolation function returned by `interp1d`\n",
    "\n",
    "#n_matches = 29\n",
    "corr_table = np.zeros([len(exp_labels),n_baden_types]) \n",
    "corr_match = np.zeros([len(exp_labels),n_baden_types])\n",
    "corr_match_vals = np.zeros([len(exp_labels),n_baden_types])\n",
    "delta_match = np.zeros([len(exp_labels),n_baden_types+1])\n",
    "\n",
    "# For each of our generated calcium type we compute the corr coef with each Baden type\n",
    "for i in exp_labels:\n",
    "    for j in np.arange(n_baden_types):\n",
    "        corr_table[i,j] = np.corrcoef(Exp_common[i], Baden_common[j])[0,1]\n",
    "    # For each calcium type, sort de corr coefs in descending order (max first)\n",
    "    corr_match[i,:] = np.flip(np.argsort(corr_table[i,:])[-32:]) # sorted indices\n",
    "    corr_match_vals[i,:] = corr_table[i,:][np.flip([np.argsort(corr_table[i,:])[-32:]])] # sorted corr coef values\n",
    "    # Select the 10 larger corr coefs and store the difference with the next one\n",
    "    for j in np.arange(10):\n",
    "        #each row is one of our clusters and each column is the corr.coeff of that cluster with a Baden one. Decreasing order\n",
    "        delta_match[i,j] = corr_match_vals[i,j]-corr_match_vals[i,j+1]  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual selection of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clusters = range(68)\n",
    "my_clusters = [0,4,5,8,18,24,32,50,54]\n",
    "\n",
    "colors = ['orange','r','g','pink','y','gray','k','c','m','darkgray','coral','gold','plum','wheat','navy']\n",
    "#             0     1   2    3     4    5     6   7   8      9         10      11     12      13     14\n",
    "\n",
    "baden_cluster=23\n",
    "\n",
    "for my_clus in my_clusters:\n",
    "\n",
    "    i=np.where(corr_match[my_clus,:]==baden_cluster)[0][0]\n",
    "    plt.figure(figsize=(12,2))\n",
    "    plt.plot(baden_time, Baden_types[baden_cluster],lw=3)\n",
    "    plt.title('{}, {}'.format(euler_labels[baden_cluster], my_clus))\n",
    "    \n",
    "    plt.plot(time, Exp_types_Ca[my_clus],color=colors[my_clus%15])\n",
    "#     plt.xlabel('match '+colors[n_match%15]+' n ' +str(n_match)+' val '+str(np.round(corr_match_vals[cluster_idx][n_match]*100)) )\n",
    "#     print('match',str(n_match),' ',j, 'Baden ',i)\n",
    "#----------------------------------------------------\n",
    "# manual_selection = corr_match[:,0].astype('int')\n",
    "\n",
    "# manual_selection=np.zeros()\n",
    "\n",
    "# manual_selection[5] = corr_match[5,5]\n",
    "# manual_selection[6] = corr_match[6,14]\n",
    "# manual_selection[7] = corr_match[7,9]\n",
    "# manual_selection[9] = corr_match[9,3]\n",
    "# manual_selection[10] = corr_match[10,21]\n",
    "# manual_selection[13] = corr_match[13,12]\n",
    "# manual_selection[14] = corr_match[14,15]\n",
    "# manual_selection[15] = corr_match[15,15]\n",
    "# manual_selection[18] = corr_match[18,4]\n",
    "# manual_selection[19] = corr_match[19,9]\n",
    "# manual_selection[20] = corr_match[20,4]\n",
    "# manual_selection[22] = corr_match[22,12]\n",
    "# manual_selection[25] = corr_match[25,2]\n",
    "# manual_selection[27] = corr_match[27,10]\n",
    "# manual_selection[29] = corr_match[29,1]\n",
    "# manual_selection[32] = corr_match[32,10]\n",
    "\n",
    "# manual_selection[37] = corr_match[37,25]\n",
    "# manual_selection[38] = corr_match[38,13]\n",
    "# manual_selection[41] = corr_match[41,18]\n",
    "# manual_selection[46] = corr_match[46,1]\n",
    "# manual_selection[64] = corr_match[64,13]\n",
    "# manual_selection[66] = corr_match[66,3]\n",
    "# manual_selection[67] = corr_match[67,21]\n",
    "# manual_selection[68] = corr_match[68,16]\n",
    "# manual_selection[69] = corr_match[69,5]\n",
    "# manual_selection[70] = corr_match[70,1]\n",
    "# manual_selection[72] = corr_match[72,16]\n",
    "# manual_selection[73] = corr_match[73,18]\n",
    "\n",
    "# manual_selection[74] = corr_match[74,3]\n",
    "\n",
    "# manual_selection[34] = corr_match[34,1]\n",
    "# manual_selection[62] = corr_match[62,1]\n",
    "# manual_selection[42] = corr_match[42,1]\n",
    "\n",
    "# manual_selection[21] =corr_match[21,2]\n",
    "\n",
    "# manual_selection[48] =corr_match[48,0]\n",
    "\n",
    "# #----------------------------------------\n",
    "# man_vals= np.zeros(len(exp_labels))\n",
    "# for i in exp_labels:\n",
    "#     if manual_selection[i]!=-1:\n",
    "#         match = np.where(manual_selection[i]==corr_match[i,:])[0][0]\n",
    "#         man_vals[i] = corr_match_vals[i,match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(corr_match[cluster_idx,:]==23)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to replot the cell group ID after the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to save the Euler label after the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsave = os.path.join(output_directory, '{}_cell_typing_data'.format(exp) )  \n",
    "save_obj(cell_data,fsave)\n",
    "\n",
    "# fsave = os.path.join(output_directory, 'Exp{}_clustermodel'.format(exp) )  \n",
    "# save_obj([model,psth_z],fsave)\n",
    "\n",
    "# fsave = os.path.join(output_directory, 'Exp{}_selected_cells'.format(exp) )  \n",
    "# save_obj(selected_cells,fsave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get cell templates\n",
    "\n",
    "# template_path = params.phy_directory\n",
    "\n",
    "# spike_templates = np.load(os.path.join(template_path, 'spike_templates.npy'))   #one template ref per spike\n",
    "# templates = np.load(os.path.join(template_path,\"templates.npy\"))\n",
    "# spike_clusters = np.load(os.path.join(template_path, 'spike_clusters.npy'))\n",
    "\n",
    "# plt.figure()\n",
    "# for cell_nb in (good_clusters[:]):   # Is the correct template recovered ?\n",
    "#     temp_inds=np.unique(spike_templates[spike_clusters==int(cell_nb)])\n",
    "#     # per each cell temp_inds is a list of the different templates that were assigned to this cell throughout the recording (sometimes it might not be a single one)\n",
    "    \n",
    "#     #----------------\n",
    "    \n",
    "#     maxt=0\n",
    "#     for t in temp_inds:\n",
    "#         for nt in range(templates.shape[2]):\n",
    "#             if maxt<abs(np.min(templates[t,:,nt])):\n",
    "#                 maxt=abs(np.min(templates[t,:,nt]))\n",
    "#                 el_sel=nt\n",
    "#                 temp_sel=t\n",
    "#                 # here I select the template index with the biggest spike amplitude. But each template index \n",
    "#                 # has several corresponding templates (why?). So even among those, given one template index, I\n",
    "#                 # chose the one with the biggest spike amplitude\n",
    "\n",
    "# #     plt.title(cell_nb)\n",
    "#     plt.plot(templates[temp_sel,:,el_sel])\n",
    "#     cell_data[cell_nb][\"template\"] = templates[temp_sel,:,el_sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
